
@article{gelman_garden_2013,
	title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	volume = {348},
	shorttitle = {The garden of forking paths},
	url = {http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf},
	urldate = {2023-10-24},
	journal = {Department of Statistics, Columbia University},
	author = {Gelman, Andrew and Loken, Eric},
	year = {2013},
	pages = {1--17},
}

@article{piai_statistically_2015,
	title = {Statistically comparing {EEG}/{MEG} waveforms through successive significant univariate tests: {How} bad can it be?},
	volume = {52},
	copyright = {© 2014 Society for Psychophysiological Research},
	issn = {1469-8986},
	shorttitle = {Statistically comparing {EEG}/{MEG} waveforms through successive significant univariate tests},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.12335},
	doi = {10.1111/psyp.12335},
	abstract = {When making statistical comparisons, the temporal dimension of the EEG signal introduces problems. Guthrie and Buchwald (1991) proposed a formally correct statistical approach that deals with these problems: comparing waveforms by counting the number of successive significant univariate tests and then contrasting this number to a well-chosen critical value. However, in the literature, this method is often used inappropriately. Using real EEG data and Monte Carlo simulations, we examined the problems associated with the incorrect use of this approach under circumstances often encountered in the literature. Our results show inflated false-positive or false-negative rates depending on parameters of the data, including filtering. Our findings suggest that most applications of this method result in an inappropriate familywise error rate control. Solutions and alternative methods are discussed.},
	language = {en},
	number = {3},
	urldate = {2023-10-24},
	journal = {Psychophysiology},
	author = {Piai, Vitória and Dahlslätt, Kristoffer and Maris, Eric},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.12335},
	keywords = {Autocorrelation, EEG/ERP, Multiple comparisons problem, Statistics, Successive univariate tests},
	pages = {440--443},
}

@article{kriegeskorte_circular_2009,
	title = {Circular analysis in systems neuroscience: the dangers of double dipping},
	volume = {12},
	copyright = {2009 Springer Nature America, Inc.},
	issn = {1546-1726},
	shorttitle = {Circular analysis in systems neuroscience},
	url = {https://www.nature.com/articles/nn.2303},
	doi = {10.1038/nn.2303},
	abstract = {This perspective illustrates some of the problems involved in analyzing the complex data yielded by systems neuroscience techniques, such as brain imaging and electrophysiology. Specifically, when test statistics are not independent of the selection criteria, common analyses can produce spurious results. The authors suggest ways to avoid such errors.},
	language = {en},
	number = {5},
	urldate = {2023-10-24},
	journal = {Nature Neuroscience},
	author = {Kriegeskorte, Nikolaus and Simmons, W. Kyle and Bellgowan, Patrick S. F. and Baker, Chris I.},
	month = may,
	year = {2009},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general},
	pages = {535--540},
}

@article{szucs_tutorial_2016,
	title = {A {Tutorial} on {Hunting} {Statistical} {Significance} by {Chasing} {N}},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01444},
	abstract = {There is increasing concern about the replicability of studies in psychology and cognitive neuroscience. Hidden data dredging (also called p-hacking) is a major contributor to this crisis because it substantially increases Type I error resulting in a much larger proportion of false positive findings than the usually expected 5\%. In order to build better intuition to avoid, detect and criticize some typical problems, here I systematically illustrate the large impact of some easy to implement and so, perhaps frequent data dredging techniques on boosting false positive findings. I illustrate several forms of two special cases of data dredging. First, researchers may violate the data collection stopping rules of null hypothesis significance testing by repeatedly checking for statistical significance with various numbers of participants. Second, researchers may group participants post hoc along potential but unplanned independent grouping variables. The first approach ‘hacks’ the number of participants in studies, the second approach ‘hacks’ the number of variables in the analysis. I demonstrate the high amount of false positive findings generated by these techniques with data from true null distributions. I also illustrate that it is extremely easy to introduce strong bias into data by very mild selection and re-testing. Similar, usually undocumented data dredging steps can easily lead to having 20–50\%, or more false positives.},
	urldate = {2023-10-24},
	journal = {Frontiers in Psychology},
	author = {Szucs, Denes},
	year = {2016},
}

@article{Simmons2011,
	title = {False-positive psychology: {Undisclosed} flexibility in data collection and analysis allows presenting anything as significant},
	volume = {22},
	issn = {14679280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤.05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process. © The Author(s) 2011.},
	number = {11},
	urldate = {2020-07-31},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pmid = {22006061},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {disclosure, methodology, motivated reasoning, publication},
	pages = {1359--1366},
}

@article{nuzzo_scientific_2014,
	title = {Scientific method: {Statistical} errors},
	volume = {506},
	copyright = {2014 Springer Nature Limited},
	issn = {1476-4687},
	shorttitle = {Scientific method},
	url = {https://www.nature.com/articles/506150a},
	doi = {10.1038/506150a},
	abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
	language = {en},
	number = {7487},
	urldate = {2023-10-17},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = feb,
	year = {2014},
	note = {Number: 7487
Publisher: Nature Publishing Group},
	keywords = {Lab life, Mathematics and computing, Medical research, Publishing},
	pages = {150--152},
}

@misc{mineault_good_2021,
	title = {The {Good} {Research} {Code} {Handbook}},
	url = {https://zenodo.org/record/5796873},
	abstract = {patrickmineault/codebook: 1.0.0. Initial release on Dec 17th, 2021, plus hotpatches for style and typos.},
	urldate = {2023-10-17},
	publisher = {Zenodo},
	author = {Mineault, Patrick and community, The Good Research Code Handbook},
	month = dec,
	year = {2021},
}

@article{wing_computational_2006,
	title = {Computational thinking},
	volume = {49},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1118178.1118215},
	doi = {10.1145/1118178.1118215},
	abstract = {It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.},
	language = {en},
	number = {3},
	urldate = {2023-10-17},
	journal = {Communications of the ACM},
	author = {Wing, Jeannette M.},
	month = mar,
	year = {2006},
	pages = {33--35},
}

@book{james_gareth_introduction_2023,
	address = {Cham},
	edition = {1st ed. 2023.},
	series = {Springer {Texts} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Learning} with {Applications} in {Python}},
	isbn = {978-3-031-38747-0},
	abstract = {by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor., An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance, marketing, and astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. This book is targeted at statisticians and non-statisticians alike, who wish to use cutting-edge statistical learning techniques to analyze their data. Four of the authors co-wrote An Introduction to Statistical Learning, With Applications in R (ISLR), which has become a mainstay of undergraduate and graduate classrooms worldwide, as well as an important reference book for data scientists. One of the keys to its success was that each chapter contains a tutorial on implementing the analyses and methods presented in the R scientific computing environment. However, in recent years Python has become a popular language for data science, and there has been increasing demand for a Python-based alternative to ISLR. Hence, this book (ISLP) covers the same materials as ISLR but with labs implemented in Python. These labs will be useful both for Python novices, as well as experienced users.},
	language = {eng},
	publisher = {Springer International Publishing},
	author = {{James Gareth} and {Witten Daniela} and {Hastie Trevor} and {Tibshirani Robert} and {Taylor Jonathan}},
	year = {2023},
	doi = {10.1007/978-3-031-38747-0},
	keywords = {Applied Statistics, Data processing, Mathematical statistics, Statistical Theory and Methods, Statistics, Statistics and Computing},
}

@book{efron_bradley_computer_2016,
	address = {New York},
	series = {Institute of {Mathematical} {Statistics} monographs},
	title = {Computer age statistical inference: algorithms, evidence, and data science},
	isbn = {978-1-316-57653-3},
	shorttitle = {Computer age statistical inference},
	abstract = {Bradley Efron, Trevor Hastie., Title from publisher's bibliographic system (viewed on 04 Jul 2016)., The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
	language = {eng},
	number = {5},
	publisher = {Cambridge University Press},
	author = {{Efron Bradley} and {Hastie Trevor}},
	year = {2016},
	keywords = {Data processing, Mathematical statistics},
}

@book{field_andy_p_adventure_2016,
	address = {London ;Thousand Oaks, California},
	title = {An adventure in statistics: the reality enigma},
	isbn = {978-1-4462-1044-4},
	shorttitle = {An adventure in statistics},
	abstract = {Andy Field., Once again, bestselling, award-winning author and teacher Andy Field hasn't just broken the traditional textbook mould with his new novel/textbook, he has forged in the fire of his imagination the only statistics book on the market with a terrifying probability bridge, zombies and talking cats!.},
	language = {eng},
	publisher = {SAGE Publications},
	author = {{Field Andy P}},
	year = {2016},
	keywords = {Lærebøger, Mathematics, Statistics, Statistik},
}

@book{efron_b_introduction_1993,
	series = {Monographs on {Statistics} and {Applied} {Probability} 57.},
	title = {An {Introduction} to the {Bootstrap}},
	isbn = {978-0-412-04231-7},
	abstract = {Efron, B,; Tibshirani, R.J.},
	language = {eng},
	publisher = {Chapman \& Hall},
	author = {{Efron B} and {Tibshirani RJ} and {Efron Bradley} and {Tibshirani R J}},
	year = {1993},
	keywords = {Bootstrap (Statistics), Statistisk analyse, regressionsmodeller, sandsynlighed, statistik},
}

@book{good_permutation_2004,
	address = {New York, NY},
	edition = {3},
	title = {Permutation, {Parametric}, and {Bootstrap} {Tests} of {Hypotheses}},
	isbn = {978-0-387-20279-2},
	language = {eng},
	publisher = {Springer},
	author = {Good, Phillip I.},
	year = {2004},
	keywords = {Resampling (Statistics)},
}

@book{poldrack_statistical_nodate,
	title = {Statistical {Thinking} for the 21st {Century}},
	url = {https://statsthinking21.github.io/statsthinking21-core-site/index.html},
	abstract = {A book about statistics.},
	urldate = {2023-10-17},
	author = {Poldrack, Copyright 2019 Russell A.},
}

@article{szucs_tutorial_2016-1,
	title = {A {Tutorial} on {Hunting} {Statistical} {Significance} by {Chasing} {N}},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01444},
	abstract = {There is increasing concern about the replicability of studies in psychology and cognitive neuroscience. Hidden data dredging (also called p-hacking) is a major contributor to this crisis because it substantially increases Type I error resulting in a much larger proportion of false positive findings than the usually expected 5\%. In order to build better intuition to avoid, detect and criticize some typical problems, here I systematically illustrate the large impact of some easy to implement and so, perhaps frequent data dredging techniques on boosting false positive findings. I illustrate several forms of two special cases of data dredging. First, researchers may violate the data collection stopping rules of null hypothesis significance testing by repeatedly checking for statistical significance with various numbers of participants. Second, researchers may group participants post hoc along potential but unplanned independent grouping variables. The first approach ‘hacks’ the number of participants in studies, the second approach ‘hacks’ the number of variables in the analysis. I demonstrate the high amount of false positive findings generated by these techniques with data from true null distributions. I also illustrate that it is extremely easy to introduce strong bias into data by very mild selection and re-testing. Similar, usually undocumented data dredging steps can easily lead to having 20–50\%, or more false positives.},
	urldate = {2023-10-17},
	journal = {Frontiers in Psychology},
	author = {Szucs, Denes},
	year = {2016},
}

@article{nuzzo_how_2015,
	title = {How scientists fool themselves – and how they can stop},
	volume = {526},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/526182a},
	doi = {10.1038/526182a},
	abstract = {Humans are remarkably good at self-deception. But growing concern about reproducibility is driving many researchers to seek ways to fight their own worst instincts.},
	language = {en},
	number = {7572},
	urldate = {2023-10-17},
	journal = {Nature},
	author = {Nuzzo, Regina},
	month = oct,
	year = {2015},
	note = {Number: 7572
Publisher: Nature Publishing Group},
	keywords = {Lab life, Research management},
	pages = {182--185},
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2023-10-17},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {289--300},
}

@article{banerjee_hypothesis_2009,
	title = {Hypothesis testing, type {I} and type {II} errors},
	volume = {18},
	issn = {0972-6748},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/},
	doi = {10.4103/0972-6748.62274},
	abstract = {Hypothesis testing is an important activity of empirical research and evidence-based medicine. A well worked up hypothesis is half the answer to the research question. For this, both knowledge of the subject derived from extensive review of the literature and working knowledge of basic statistical concepts are desirable. The present paper discusses the methods of working up a good hypothesis and statistical concepts of hypothesis testing.},
	number = {2},
	urldate = {2023-10-17},
	journal = {Industrial Psychiatry Journal},
	author = {Banerjee, Amitav and Chitnis, U. B. and Jadhav, S. L. and Bhawalkar, J. S. and Chaudhury, S.},
	year = {2009},
	pmid = {21180491},
	pmcid = {PMC2996198},
	pages = {127--131},
}

@techreport{pernet_null_2017,
	title = {Null hypothesis significance testing: a guide to commonly misunderstood concepts and recommendations for good practice},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Null hypothesis significance testing},
	url = {https://f1000research.com/articles/4-621},
	abstract = {Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences. In this short guide, I first summarize the concepts behind the method, distinguishing test of significance (Fisher) and test of acceptance (Newman-Pearson) and point to common interpretation errors regarding the p-value. I then present the related concepts of confidence intervals and again point to common interpretation errors. Finally, I discuss what should be reported in which context. The goal is to clarify concepts to avoid interpretation errors and propose simple reporting practices.},
	language = {en},
	number = {4:621},
	urldate = {2023-10-17},
	institution = {F1000Research},
	author = {Pernet, Cyril},
	month = oct,
	year = {2017},
	doi = {10.12688/f1000research.6963.5},
	note = {Type: article},
	keywords = {confidence intervals, null hypothesis significance testing, p-value, reporting, tutorial},
}

@article{nickerson_null_2000,
	title = {Null hypothesis significance testing: a review of an old and continuing controversy},
	volume = {5},
	issn = {1082-989X},
	shorttitle = {Null hypothesis significance testing},
	doi = {10.1037/1082-989x.5.2.241},
	abstract = {Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data.},
	language = {eng},
	number = {2},
	journal = {Psychological Methods},
	author = {Nickerson, R. S.},
	month = jun,
	year = {2000},
	pmid = {10937333},
	keywords = {Data Interpretation, Statistical, Humans, Psychology, Experimental, Psychometrics, Reproducibility of Results},
	pages = {241--301},
}

@misc{weisstein_bonferroni_nodate,
	type = {Text},
	title = {Bonferroni {Correction}},
	copyright = {Copyright 1999-2023 Wolfram Research, Inc.  See https://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
	url = {https://mathworld.wolfram.com/},
	abstract = {The Bonferroni correction is a multiple-comparison correction used when several dependent or independent statistical tests are being performed simultaneously (since while a given alpha value alpha may be appropriate for each individual comparison, it is not for the set of all comparisons). In order to avoid a lot of spurious positives, the alpha value needs to be lowered to account for the number of comparisons being performed. The simplest and most conservative approach is the Bonferroni...},
	language = {en},
	urldate = {2023-10-17},
	author = {Weisstein, Eric W.},
	note = {Publisher: Wolfram Research, Inc.},
}

@article{noauthor_lets_2015,
	title = {Let’s think about cognitive bias},
	volume = {526},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/526163a},
	doi = {10.1038/526163a},
	abstract = {The human brain’s habit of finding what it wants to find is a key problem for research. Establishing robust methods to avoid such bias will make results more reproducible.},
	language = {en},
	number = {7572},
	urldate = {2023-10-17},
	journal = {Nature},
	month = oct,
	year = {2015},
	note = {Number: 7572
Publisher: Nature Publishing Group},
	keywords = {Research management},
	pages = {163--163},
}

@misc{charpentier_p-hacking_2015,
	type = {Billet},
	title = {p-hacking, or cheating on a p-value},
	url = {https://freakonometrics.hypotheses.org/19817},
	abstract = {Yesterday evening, I discovered some interesting slides on False-Positives, p-Hacking, Statistical Power, and Evidential Value, via @UCBITSS ‘s post on Twitter. More precisely, there was this slide on how cheating (because that’s basically what it is) to get a ‘good’ model (by targeting the p-value) As mentioned by @david\_colquhoun  one should be careful when reading the slides : some statistician … Continue reading p-hacking, or cheating on a p-value →},
	language = {en-US},
	urldate = {2023-10-17},
	journal = {Freakonometrics},
	author = {Charpentier, Arthur},
	month = jun,
	year = {2015},
}

@article{eglen_toward_2017,
	title = {Toward standard practices for sharing computer code and programs in neuroscience},
	volume = {20},
	copyright = {2017 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4550},
	doi = {10.1038/nn.4550},
	abstract = {Computational techniques are central in many areas of neuroscience and are relatively easy to share. This paper describes why computer programs underlying scientific publications should be shared and lists simple steps for sharing. Together with ongoing efforts in data sharing, this should aid reproducibility of research.},
	language = {en},
	number = {6},
	urldate = {2023-10-17},
	journal = {Nature Neuroscience},
	author = {Eglen, Stephen J. and Marwick, Ben and Halchenko, Yaroslav O. and Hanke, Michael and Sufi, Shoaib and Gleeson, Padraig and Silver, R. Angus and Davison, Andrew P. and Lanyon, Linda and Abrams, Mathew and Wachtler, Thomas and Willshaw, David J. and Pouzat, Christophe and Poline, Jean-Baptiste},
	month = jun,
	year = {2017},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Scientific community},
	pages = {770--773},
}

@article{bannier_open_2021,
	title = {The {Open} {Brain} {Consent}: {Informing} research participants and obtaining consent to share brain imaging data},
	volume = {42},
	copyright = {© 2021 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.},
	issn = {1097-0193},
	shorttitle = {The {Open} {Brain} {Consent}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.25351},
	doi = {10.1002/hbm.25351},
	abstract = {Having the means to share research data openly is essential to modern science. For human research, a key aspect in this endeavor is obtaining consent from participants, not just to take part in a study, which is a basic ethical principle, but also to share their data with the scientific community. To ensure that the participants' privacy is respected, national and/or supranational regulations and laws are in place. It is, however, not always clear to researchers what the implications of those are, nor how to comply with them. The Open Brain Consent (https://open-brain-consent.readthedocs.io) is an international initiative that aims to provide researchers in the brain imaging community with information about data sharing options and tools. We present here a short history of this project and its latest developments, and share pointers to consent forms, including a template consent form that is compliant with the EU general data protection regulation. We also share pointers to an associated data user agreement that is not only useful in the EU context, but also for any researchers dealing with personal (clinical) data elsewhere.},
	language = {en},
	number = {7},
	urldate = {2023-06-27},
	journal = {Human Brain Mapping},
	author = {Bannier, Elise and Barker, Gareth and Borghesani, Valentina and Broeckx, Nils and Clement, Patricia and Emblem, Kyrre E. and Ghosh, Satrajit and Glerean, Enrico and Gorgolewski, Krzysztof J. and Havu, Marko and Halchenko, Yaroslav O. and Herholz, Peer and Hespel, Anne and Heunis, Stephan and Hu, Yue and Hu, Chuan-Peng and Huijser, Dorien and de la Iglesia Vayá, María and Jancalek, Radim and Katsaros, Vasileios K. and Kieseler, Marie-Luise and Maumet, Camille and Moreau, Clara A. and Mutsaerts, Henk-Jan and Oostenveld, Robert and Ozturk-Isik, Esin and Pascual Leone Espinosa, Nicolas and Pellman, John and Pernet, Cyril R and Pizzini, Francesca Benedetta and Trbalić, Amira Šerifović and Toussaint, Paule-Joanne and Visconti di Oleggio Castello, Matteo and Wang, Fengjuan and Wang, Cheng and Zhu, Hua},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.25351},
	keywords = {brain imaging, general data protection regulation, informed consent},
	pages = {1945--1951},
}

@article{allen_provenance_2010,
	title = {Provenance {Capture} and {Use}: {A} {Practical} {Guide}},
	abstract = {There is a widespread recognition across MITRE’s sponsors of the importance of capturing the provenance of information (sometimes called lineage or pedigree). However, the technology for supporting capture and usage of provenance is relatively immature. While there has been much research, few commercial capabilities exist. In addition, there is neither a commonly understood concept of operations nor established best practices for how to capture and use provenance information in a consistent and principled way. This document captures lessons learned from the IM-PLUS project, which is prototyping a provenance capability that synthesizes prior research; the project is also applying the prototype to government application scenarios spanning defense, homeland security, and bio-surveillance. We describe desirable features of a provenance capability and trade-offs among alternate provenance approaches. The target audience is systems engineers and information architects advising our sponsors on how to improve their information management.},
	language = {en},
	author = {Allen, M David and Seligman, Len and Blaustein, Barbara and Chapman, Adriane},
	year = {2010},
}

@misc{vukovic_setting_nodate,
	title = {Setting up an {Organised} {Folder} {Structure} for {Research} {Projects}},
	url = {http://nikola.me/folder_structure.html},
	author = {Vukovic, Nikola},
}

@misc{noauthor_gin-tonic_nodate,
	title = {{GIN}-{TONIC}: a digital shelf for your research files},
}

@misc{glerean_project_nodate,
	title = {Project management == {Data} management},
	url = {https://eglerean.wordpress.com/2017/05/24/project-management-data-management/},
	author = {Glerean, Enrico},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Abstract
            Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie Du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
}

@article{fernandez-juricic_why_2021,
	title = {Why sharing data and code during peer review can enhance behavioral ecology research},
	volume = {75},
	issn = {0340-5443, 1432-0762},
	url = {https://link.springer.com/10.1007/s00265-021-03036-x},
	doi = {10.1007/s00265-021-03036-x},
	language = {en},
	number = {7},
	urldate = {2023-06-27},
	journal = {Behavioral Ecology and Sociobiology},
	author = {Fernández-Juricic, Esteban},
	month = jul,
	year = {2021},
	pages = {103, s00265--021--03036--x},
}

@article{galton_biometry_1901,
	title = {Biometry},
	volume = {1},
	journal = {Biometrika},
	author = {Galton, Francis},
	year = {1901},
	pages = {7--10},
}

@article{king_publication_2006,
	title = {Publication, {Publication}},
	language = {en},
	author = {King, Gary},
	year = {2006},
}

@misc{noauthor_survey_nodate,
	title = {A survey on how preregistration affects the research workflow: better science but more work {\textbar} {Royal} {Society} {Open} {Science}},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.211997},
	urldate = {2023-06-27},
}

@book{fisher_design_1971,
	edition = {9},
	title = {The design of experiments},
	author = {Fisher, Ronald A.},
	year = {1971},
}

@article{markowetz_five_2015,
	title = {Five selfish reasons to work reproducibly},
	volume = {16},
	issn = {1474-760X},
	url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7},
	doi = {10.1186/s13059-015-0850-7},
	abstract = {And so, my fellow scientists: ask not what you can do for reproducibility; ask what reproducibility can do for you! Here, I present five reasons why working reproducibly pays off in the long run and is in the self-interest of every ambitious, career-oriented scientist.},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {Genome Biology},
	author = {Markowetz, Florian},
	month = dec,
	year = {2015},
	pages = {274},
}

@article{colavizza_citation_2020,
	title = {The citation advantage of linking publications to research data},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0230416},
	doi = {10.1371/journal.pone.0230416},
	abstract = {Efforts to make research results open and reproducible are increasingly reflected by journal policies encouraging or mandating authors to provide data availability statements. As a consequence of this, there has been a strong uptake of data availability statements in recent literature. Nevertheless, it is still unclear what proportion of these statements actually contain well-formed links to data, for example via a URL or permanent identifier, and if there is an added value in providing such links. We consider 531, 889 journal articles published by PLOS and BMC, develop an automatic system for labelling their data availability statements according to four categories based on their content and the type of data availability they display, and finally analyze the citation advantage of different statement categories via regression. We find that, following mandated publisher policies, data availability statements become very common. In 2018 93.7\% of 21,793 PLOS articles and 88.2\% of 31,956 BMC articles had data availability statements. Data availability statements containing a link to data in a repository—rather than being available on request or included as supporting information files—are a fraction of the total. In 2017 and 2018, 20.8\% of PLOS publications and 12.2\% of BMC publications provided DAS containing a link to data in a repository. We also find an association between articles that include statements that link to data in a repository and up to 25.36\% (± 1.07\%) higher citation impact on average, using a citation prediction model. We discuss the potential implications of these results for authors (researchers) and journal publishers who make the effort of sharing their data in repositories. All our data and code are made available in order to reproduce and extend our results.},
	language = {en},
	number = {4},
	urldate = {2023-06-27},
	journal = {PLOS ONE},
	author = {Colavizza, Giovanni and Hrynaszkiewicz, Iain and Staden, Isla and Whitaker, Kirstie and McGillivray, Barbara},
	editor = {Wicherts, Jelte M.},
	month = apr,
	year = {2020},
	pages = {e0230416},
}

@article{pernet_improving_2015,
	title = {Improving functional magnetic resonance imaging reproducibility},
	volume = {4},
	issn = {2047-217X},
	url = {https://academic.oup.com/gigascience/article-lookup/doi/10.1186/s13742-015-0055-8},
	doi = {10.1186/s13742-015-0055-8},
	abstract = {Background: The ability to replicate an entire experiment is crucial to the scientific method. With the development of more and more complex paradigms, and the variety of analysis techniques available, fMRI studies are becoming harder to reproduce.
Results: In this article, we aim to provide practical advice to fMRI researchers not versed in computing, in order to make studies more reproducible. All of these steps require researchers to move towards a more open science, in which all aspects of the experimental method are documented and shared.
Conclusion: Only by sharing experiments, data, metadata, derived data and analysis workflows will neuroimaging establish itself as a true data science.},
	language = {en},
	number = {1},
	urldate = {2023-06-27},
	journal = {GigaScience},
	author = {Pernet, Cyril and Poline, Jean-Baptiste},
	month = dec,
	year = {2015},
	pages = {15},
}

@misc{whitacker_turing_nodate,
	title = {The {Turing} {Way}: {A} {Handbook} for {Reproducible} {Data} {Science}},
	url = {https://the-turing-way.netlify.app/index.html},
	author = {Whitacker, Kirstie},
	note = {https://doi.org/10.5281/zenodo.7625728},
}

@article{Poldrack2017a,
	title = {Scanning the horizon: towards transparent and reproducible neuroimaging research},
	volume = {18},
	issn = {1471-003X},
	url = {http://www.nature.com/doifinder/10.1038/nrn.2016.167},
	doi = {10.1038/nrn.2016.167},
	abstract = {Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors, and lack of direct replication apply to many fields, but perhaps particularly to fMRI. Here we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful answers to neuroscientific questions.},
	number = {2},
	journal = {Nature Reviews Neuroscience},
	author = {Poldrack, Russell A. and Baker, Chris I. and Durnez, Joke and Gorgolewski, Krzysztof J. and Matthews, Paul M. and Munafò, Marcus R. and Nichols, Thomas E. and Poline, Jean-Baptiste and Vul, Edward and Yarkoni, Tal},
	year = {2017},
	pmid = {28053326},
	note = {ISBN: 1471-003X},
	keywords = {★},
	pages = {115--126},
}

@techreport{Poldrack,
	title = {Reproducibility in neuroimaging: {Challenges} and solutions},
	author = {Poldrack, Russell},
}

@article{Poldrack2020,
	title = {Establishment of {Best} {Practices} for {Evidence} for {Prediction}: {A} {Review}},
	issn = {2168622X},
	doi = {10.1001/jamapsychiatry.2019.3671},
	abstract = {Importance: Great interest exists in identifying methods to predict neuropsychiatric disease states and treatment outcomes from high-dimensional data, including neuroimaging and genomics data. The goal of this review is to highlight several potential problems that can arise in studies that aim to establish prediction. Observations: A number of neuroimaging studies have claimed to establish prediction while establishing only correlation, which is an inappropriate use of the statistical meaning of prediction. Statistical associations do not necessarily imply the ability to make predictions in a generalized manner; establishing evidence for prediction thus requires testing of the model on data separate from those used to estimate the model's parameters. This article discusses various measures of predictive performance and the limitations of some commonly used measures, with a focus on the importance of using multiple measures when assessing performance. For classification, the area under the receiver operating characteristic curve is an appropriate measure; for regression analysis, correlation should be avoided, and median absolute error is preferred. Conclusions and Relevance: To ensure accurate estimates of predictive validity, the recommended best practices for predictive modeling include the following: (1) in-sample model fit indices should not be reported as evidence for predictive accuracy, (2) the cross-validation procedure should encompass all operations applied to the data, (3) prediction analyses should not be performed with samples smaller than several hundred observations, (4) multiple measures of prediction accuracy should be examined and reported, (5) the coefficient of determination should be computed using the sums of squares formulation and not the correlation coefficient, and (6) k-fold cross-validation rather than leave-one-out cross-validation should be used..},
	journal = {JAMA Psychiatry},
	author = {Poldrack, Russell A. and Huckins, Grace and Varoquaux, Gael},
	year = {2020},
	pmid = {31774490},
}

@article{Halchenko2021,
	title = {{DataLad}: distributed system for joint management of code, data, and their relationship},
	volume = {6},
	doi = {10.21105/JOSS.03262},
	abstract = {… technical challenges of data management, data sharing, and digital provenance collection across the life cycle of digital objects. DataLad aims to make data management as easy as …},
	number = {63},
	urldate = {2022-04-06},
	journal = {Journal of Open Source Software},
	author = {Halchenko, Yaroslav and Meyer, Kyle and Poldrack, Benjamin and Solanky, Debanjum and Wagner, Adina and Gors, Jason and MacFarlane, Dave and Pustina, Dorian and Sochat, Vanessa and Ghosh, Satrajit and Mönch, Christian and Markiewicz, Christopher and Waite, Laura and Shlyakhter, Ilya and de la Vega, Alejandro and Hayashi, Soichi and Häusler, Christian and Poline, Jean-Baptiste and Kadelka, Tobias and Skytén, Kusti and Jarecka, Dorota and Kennedy, David and Strauss, Ted and Cieslak, Matt and Vavra, Peter and Ioanas, Horea-Ioan and Schneider, Robin and Pflüger, Mika and Haxby, James and Eickhoff, Simon and Hanke, Michael},
	month = jul,
	year = {2021},
	note = {Publisher: The Open Journal},
	pages = {3262},
}

@article{Poldrack2014,
	title = {Making big data open: {Data} sharing in neuroimaging},
	volume = {17},
	issn = {15461726},
	doi = {10.1038/NN.3818},
	abstract = {In the last decade, major advances have been made in the availability of shared neuroimaging data, such that there are more than 8,000 shared MRI (magnetic resonance imaging) data sets available online. Here we outline the state of data sharing for task-based functional MRI (fMRI) data, with a focus on various forms of data and their relative utility for subsequent analyses. We also discuss challenges to the future success of data sharing and highlight the ethical argument that data sharing may be necessary to maximize the contribution of human subjects.},
	number = {11},
	urldate = {2022-05-01},
	journal = {Nature Neuroscience},
	author = {Poldrack, Russell A. and Gorgolewski, Krzysztof J.},
	month = oct,
	year = {2014},
	pmid = {25349916},
	note = {Publisher: Nature Publishing Group},
	pages = {1510--1517},
}

@article{Poldrack2019,
	title = {The {Costs} of {Reproducibility}},
	volume = {101},
	issn = {0896-6273},
	doi = {10.1016/J.NEURON.2018.11.030},
	abstract = {Improving the reproducibility of neuroscience research is of great concern, especially to early-career researchers (ECRs). Here I outline the potential costs for ECRs in adopting practices to improve reproducibility. I highlight the ways in which ECRs can achieve their career goals while doing better science and the need for established researchers to support them in these efforts.},
	number = {1},
	urldate = {2022-07-25},
	journal = {Neuron},
	author = {Poldrack, Russell A.},
	month = jan,
	year = {2019},
	pmid = {30605654},
	note = {Publisher: Cell Press},
	pages = {11--14},
}

@article{goodman_what_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	issn = {1946-6234, 1946-6242},
	url = {https://www.science.org/doi/10.1126/scitranslmed.aaf5027},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.
          , 
            The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”},
	language = {en},
	number = {341},
	urldate = {2023-06-27},
	journal = {Science Translational Medicine},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	month = jun,
	year = {2016},
}

@article{kenett_clarifying_2015,
	title = {Clarifying the terminology that describes scientific reproducibility},
	volume = {12},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/nmeth.3489},
	doi = {10.1038/nmeth.3489},
	language = {en},
	number = {8},
	urldate = {2023-06-27},
	journal = {Nature Methods},
	author = {Kenett, Ron S and Shmueli, Galit},
	month = aug,
	year = {2015},
	pages = {699--699},
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	doi = {10.1038/533452a},
	journal = {Nature},
	author = {Baker, M},
	year = {2016},
}

@article{Button2019,
	title = {Double-dipping revisited},
	volume = {22},
	issn = {15461726},
	url = {http://dx.doi.org/10.1038/s41593-019-0398-z},
	doi = {10.1038/s41593-019-0398-z},
	abstract = {Robust conclusions require rigorous statistics. In 2009 a seminal paper described the dangers and prevalence of double-dipping in neuroscience. Ten years on, I consider progress toward statistical rigor in neuroimaging.},
	number = {5},
	journal = {Nature Neuroscience},
	author = {Button, Katherine S.},
	year = {2019},
	note = {Publisher: Springer US},
	pages = {688--690},
}

@article{Button2013,
	title = {Power failure: {Why} small sample size undermines the reliability of neuroscience},
	volume = {14},
	issn = {1471003X},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles. © 2013 Macmillan Publishers Limited. All rights reserved.},
	number = {5},
	urldate = {2020-07-31},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P.A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S.J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	pmid = {23571845},
	pages = {365--376},
}

@article{Munafo2017a,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {23973374},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: Methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	number = {1},
	urldate = {2020-07-31},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V.M. and Button, Katherine S. and Chambers, Christopher D. and Percie Du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric Jan and Ware, Jennifer J. and Ioannidis, John P.A.},
	month = jan,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {★},
}

@article{baker_over_2015,
	title = {Over half of psychology studies fail reproducibility test},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature.2015.18248},
	doi = {10.1038/nature.2015.18248},
	abstract = {Largest replication study to date casts doubt on many published positive results.},
	language = {en},
	urldate = {2023-01-23},
	journal = {Nature},
	author = {Baker, Monya},
	month = aug,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Authorship, Psychology, Research management},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
            
              One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts
              et al.
              describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
            
            
              Science
              , this issue
              10.1126/science.aac4716
            
          , 
            A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
          , 
            
              INTRODUCTION
              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
            
            
              RATIONALE
              There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
            
            
              RESULTS
              
                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and
                P
                values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (
                M
                r
                = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (
                M
                r
                = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (
                P
                {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
              
            
            
              CONCLUSION
              
                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original
                P
                value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.
              
              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.
              
                
                  Original study effect size versus replication effect size (correlation coefficients).
                  Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
                
                
              
            
          , 
            Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	language = {en},
	number = {6251},
	urldate = {2023-06-27},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pages = {aac4716},
}

@article{errington_investigating_2021,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = {10},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/71601},
	doi = {10.7554/eLife.71601},
	abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-­ impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
	language = {en},
	urldate = {2023-06-27},
	journal = {eLife},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	month = dec,
	year = {2021},
	pages = {e71601},
}

@article{kleinert_how_2014,
	title = {How should medical science change?},
	volume = {383},
	issn = {01406736},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673613626781},
	doi = {10.1016/S0140-6736(13)62678-1},
	language = {en},
	number = {9913},
	urldate = {2023-06-27},
	journal = {The Lancet},
	author = {Kleinert, Sabine and Horton, Richard},
	month = jan,
	year = {2014},
	pages = {197--198},
}

@article{macleod_biomedical_2014,
	title = {Biomedical research: increasing value, reducing waste},
	volume = {383},
	issn = {01406736},
	shorttitle = {Biomedical research},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0140673613623296},
	doi = {10.1016/S0140-6736(13)62329-6},
	language = {en},
	number = {9912},
	urldate = {2023-06-27},
	journal = {The Lancet},
	author = {Macleod, Malcolm R and Michie, Susan and Roberts, Ian and Dirnagl, Ulrich and Chalmers, Iain and Ioannidis, John P A and Salman, Rustam Al-Shahi and Chan, An-Wen and Glasziou, Paul},
	month = jan,
	year = {2014},
	pages = {101--104},
}

@article{bustin_reproducibility_2014,
	title = {The reproducibility of biomedical research: {Sleepers} awake!},
	volume = {2},
	issn = {22147535},
	shorttitle = {The reproducibility of biomedical research},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2214753515000030},
	doi = {10.1016/j.bdq.2015.01.002},
	language = {en},
	urldate = {2023-06-27},
	journal = {Biomolecular Detection and Quantification},
	author = {Bustin, Stephen A.},
	month = dec,
	year = {2014},
	pages = {35--42},
}

@article{baker_first_2015,
	title = {First results from psychology’s largest reproducibility test},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature.2015.17433},
	doi = {10.1038/nature.2015.17433},
	language = {en},
	urldate = {2023-06-27},
	journal = {Nature},
	author = {Baker, Monya},
	month = apr,
	year = {2015},
	pages = {nature.2015.17433},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research ﬁndings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientiﬁc ﬁeld. In this framework, a research ﬁnding is less likely to be true when the studies conducted in a ﬁeld are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater ﬂexibility in designs, deﬁnitions, outcomes, and analytical modes; when there is greater ﬁnancial and other interest and prejudice; and when more teams are involved in a scientiﬁc ﬁeld in chase of statistical signiﬁcance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientiﬁc ﬁelds, claimed research ﬁndings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2023-06-27},
	journal = {PLoS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
}

@misc{nosek_breakdown_nodate,
	title = {Breakdown of the percentage of papers mentioning preregistration in sub-disciplines of psychology.},
	url = {https://nerdculture.de/@briannosek/110533439789757366},
	author = {Nosek, Brian},
}

@article{sarafoglou_survey_2022,
	title = {A survey on how preregistration affects the research workflow: better science but more work},
	volume = {9},
	issn = {2054-5703},
	shorttitle = {A survey on how preregistration affects the research workflow},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211997},
	doi = {10.1098/rsos.211997},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
	language = {en},
	number = {7},
	urldate = {2023-06-27},
	journal = {Royal Society Open Science},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
	month = jul,
	year = {2022},
	pages = {211997},
}

@misc{blattman_preregistration_nodate,
	title = {Preregistration of clinical trials causes medicines to stop working!},
	url = {https://chrisblattman.com/blog/2016/03/01/13719/},
	author = {Blattman, Chris},
}

@article{kwok_burgeoning_nodate,
	title = {A burgeoning array of digital tools is helping researchers to document experiments with ease.},
	language = {en},
	author = {Kwok, Roberta and Kanza, S},
}
