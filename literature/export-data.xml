<?xml version="1.0" encoding="UTF-8"?>
<xml><records><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Gelman, Andrew</author><author>Loken, Eric</author></authors></contributors><titles><title>The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time</title><secondary-title>Department of Statistics, Columbia University</secondary-title><short-title>The garden of forking paths</short-title></titles><periodical><full-title>Department of Statistics, Columbia University</full-title></periodical><pages>1–17</pages><volume>348</volume><dates><year>2013</year><pub-dates><date>2013</date></pub-dates></dates><remote-database-name>Google Scholar</remote-database-name><urls><web-urls><url>http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf</url></web-urls></urls><access-date>2023-10-24 10:09:39</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Piai, Vitória</author><author>Dahlslätt, Kristoffer</author><author>Maris, Eric</author></authors></contributors><titles><title>Statistically comparing EEG/MEG waveforms through successive significant univariate tests: How bad can it be?</title><secondary-title>Psychophysiology</secondary-title><short-title>Statistically comparing EEG/MEG waveforms through successive significant univariate tests</short-title></titles><periodical><full-title>Psychophysiology</full-title></periodical><pages>440-443</pages><volume>52</volume><number>3</number><issue>3</issue><keywords><keyword>Autocorrelation</keyword><keyword>EEG/ERP</keyword><keyword>Multiple comparisons problem</keyword><keyword>Statistics</keyword><keyword>Successive univariate tests</keyword></keywords><dates><year>2015</year><pub-dates><date>2015</date></pub-dates></dates><isbn>1469-8986</isbn><electronic-resource-num>10.1111/psyp.12335</electronic-resource-num><abstract>When making statistical comparisons, the temporal dimension of the EEG signal introduces problems. Guthrie and Buchwald (1991) proposed a formally correct statistical approach that deals with these problems: comparing waveforms by counting the number of successive significant univariate tests and then contrasting this number to a well-chosen critical value. However, in the literature, this method is often used inappropriately. Using real EEG data and Monte Carlo simulations, we examined the problems associated with the incorrect use of this approach under circumstances often encountered in the literature. Our results show inflated false-positive or false-negative rates depending on parameters of the data, including filtering. Our findings suggest that most applications of this method result in an inappropriate familywise error rate control. Solutions and alternative methods are discussed.</abstract><remote-database-name>Wiley Online Library</remote-database-name><language>en</language><urls><web-urls><url>https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.12335</url></web-urls></urls><access-date>2023-10-24 09:55:27</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Kriegeskorte, Nikolaus</author><author>Simmons, W. Kyle</author><author>Bellgowan, Patrick S. F.</author><author>Baker, Chris I.</author></authors></contributors><titles><title>Circular analysis in systems neuroscience: the dangers of double dipping</title><secondary-title>Nature Neuroscience</secondary-title><short-title>Circular analysis in systems neuroscience</short-title></titles><periodical><full-title>Nature Neuroscience</full-title><abbr-1>Nat Neurosci</abbr-1></periodical><pages>535-540</pages><volume>12</volume><number>5</number><issue>5</issue><keywords><keyword>Animal Genetics and Genomics</keyword><keyword>Behavioral Sciences</keyword><keyword>Biological Techniques</keyword><keyword>Biomedicine</keyword><keyword>Neurobiology</keyword><keyword>Neurosciences</keyword><keyword>general</keyword></keywords><dates><year>2009</year><pub-dates><date>2009-05</date></pub-dates></dates><isbn>1546-1726</isbn><electronic-resource-num>10.1038/nn.2303</electronic-resource-num><abstract>This perspective illustrates some of the problems involved in analyzing the complex data yielded by systems neuroscience techniques, such as brain imaging and electrophysiology. Specifically, when test statistics are not independent of the selection criteria, common analyses can produce spurious results. The authors suggest ways to avoid such errors.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nn.2303</url></web-urls></urls><access-date>2023-10-24 09:53:26</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Szucs, Denes</author></authors></contributors><titles><title>A Tutorial on Hunting Statistical Significance by Chasing N</title><secondary-title>Frontiers in Psychology</secondary-title></titles><periodical><full-title>Frontiers in Psychology</full-title></periodical><volume>7</volume><dates><year>2016</year><pub-dates><date>2016</date></pub-dates></dates><isbn>1664-1078</isbn><abstract>There is increasing concern about the replicability of studies in psychology and cognitive neuroscience. Hidden data dredging (also called p-hacking) is a major contributor to this crisis because it substantially increases Type I error resulting in a much larger proportion of false positive findings than the usually expected 5%. In order to build better intuition to avoid, detect and criticize some typical problems, here I systematically illustrate the large impact of some easy to implement and so, perhaps frequent data dredging techniques on boosting false positive findings. I illustrate several forms of two special cases of data dredging. First, researchers may violate the data collection stopping rules of null hypothesis significance testing by repeatedly checking for statistical significance with various numbers of participants. Second, researchers may group participants post hoc along potential but unplanned independent grouping variables. The first approach ‘hacks’ the number of participants in studies, the second approach ‘hacks’ the number of variables in the analysis. I demonstrate the high amount of false positive findings generated by these techniques with data from true null distributions. I also illustrate that it is extremely easy to introduce strong bias into data by very mild selection and re-testing. Similar, usually undocumented data dredging steps can easily lead to having 20–50%, or more false positives.</abstract><remote-database-name>Frontiers</remote-database-name><urls><web-urls><url>https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01444</url></web-urls></urls><access-date>2023-10-24 09:19:27</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Simmons, Joseph P.</author><author>Nelson, Leif D.</author><author>Simonsohn, Uri</author></authors></contributors><titles><title>False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</title><secondary-title>Psychological Science</secondary-title></titles><periodical><full-title>Psychological Science</full-title></periodical><pages>1359-1366</pages><volume>22</volume><number>11</number><issue>11</issue><keywords><keyword>disclosure</keyword><keyword>methodology</keyword><keyword>motivated reasoning</keyword><keyword>publication</keyword></keywords><dates><year>2011</year><pub-dates><date>2011-11-17</date></pub-dates></dates><isbn>14679280</isbn><electronic-resource-num>10.1177/0956797611417632</electronic-resource-num><abstract>In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤.05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process. © The Author(s) 2011.</abstract><urls><web-urls><url>http://journals.sagepub.com/doi/10.1177/0956797611417632</url></web-urls></urls><access-date>2020-07-31 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Nuzzo, Regina</author></authors></contributors><titles><title>Scientific method: Statistical errors</title><secondary-title>Nature</secondary-title><short-title>Scientific method</short-title></titles><periodical><full-title>Nature</full-title></periodical><pages>150-152</pages><volume>506</volume><number>7487</number><issue>7487</issue><keywords><keyword>Lab life</keyword><keyword>Mathematics and computing</keyword><keyword>Medical research</keyword><keyword>Publishing</keyword></keywords><dates><year>2014</year><pub-dates><date>2014-02-01</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/506150a</electronic-resource-num><abstract>P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/506150a</url></web-urls></urls><access-date>2023-10-17 09:22:44</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Computer Program">9</ref-type><contributors><authors><author>Mineault, Patrick</author><author>community, The Good Research Code Handbook</author></authors></contributors><titles><title>The Good Research Code Handbook</title></titles><dates><year>2021</year><pub-dates><date>2021-12-21</date></pub-dates></dates><publisher>Zenodo</publisher><abstract>patrickmineault/codebook: 1.0.0. Initial release on Dec 17th, 2021, plus hotpatches for style and typos.</abstract><urls><web-urls><url>https://zenodo.org/record/5796873</url></web-urls></urls><access-date>2023-10-17 08:58:40</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Wing, Jeannette M.</author></authors></contributors><titles><title>Computational thinking</title><secondary-title>Communications of the ACM</secondary-title></titles><periodical><full-title>Communications of the ACM</full-title><abbr-1>Commun. ACM</abbr-1></periodical><pages>33-35</pages><volume>49</volume><number>3</number><issue>3</issue><dates><year>2006</year><pub-dates><date>03/2006</date></pub-dates></dates><isbn>0001-0782, 1557-7317</isbn><electronic-resource-num>10.1145/1118178.1118215</electronic-resource-num><abstract>It represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://dl.acm.org/doi/10.1145/1118178.1118215</url></web-urls></urls><access-date>2023-10-17 08:46:42</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>James Gareth</author><author>Witten Daniela</author><author>Hastie Trevor</author><author>Tibshirani Robert</author><author>Taylor Jonathan</author></authors></contributors><titles><title>An Introduction to Statistical Learning with Applications in Python</title><secondary-title>Springer Texts in Statistics</secondary-title></titles><edition>1st ed. 2023.</edition><keywords><keyword>Applied Statistics</keyword><keyword>Data processing</keyword><keyword>Mathematical statistics</keyword><keyword>Statistical Theory and Methods</keyword><keyword>Statistics</keyword><keyword>Statistics and Computing</keyword></keywords><dates><year>2023</year><pub-dates><date>2023</date></pub-dates></dates><pub-location>Cham</pub-location><publisher>Springer International Publishing</publisher><isbn>978-3-031-38747-0</isbn><abstract>by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor., An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance, marketing, and astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. This book is targeted at statisticians and non-statisticians alike, who wish to use cutting-edge statistical learning techniques to analyze their data. Four of the authors co-wrote An Introduction to Statistical Learning, With Applications in R (ISLR), which has become a mainstay of undergraduate and graduate classrooms worldwide, as well as an important reference book for data scientists. One of the keys to its success was that each chapter contains a tutorial on implementing the analyses and methods presented in the R scientific computing environment. However, in recent years Python has become a popular language for data science, and there has been increasing demand for a Python-based alternative to ISLR. Hence, this book (ISLP) covers the same materials as ISLR but with labs implemented in Python. These labs will be useful both for Python novices, as well as experienced users.</abstract><remote-database-name>soeg.kb.dk</remote-database-name><language>eng</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Efron Bradley</author><author>Hastie Trevor</author></authors></contributors><titles><title>Computer age statistical inference: algorithms, evidence, and data science</title><secondary-title>Institute of Mathematical Statistics monographs</secondary-title><short-title>Computer age statistical inference</short-title></titles><number>5</number><keywords><keyword>Data processing</keyword><keyword>Mathematical statistics</keyword></keywords><dates><year>2016</year><pub-dates><date>2016</date></pub-dates></dates><pub-location>New York</pub-location><publisher>Cambridge University Press</publisher><isbn>978-1-316-57653-3</isbn><abstract>Bradley Efron, Trevor Hastie., Title from publisher's bibliographic system (viewed on 04 Jul 2016)., The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.</abstract><remote-database-name>soeg.kb.dk</remote-database-name><language>eng</language><misc1>5</misc1></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Field Andy P</author></authors></contributors><titles><title>An adventure in statistics: the reality enigma</title><short-title>An adventure in statistics</short-title></titles><keywords><keyword>Lærebøger</keyword><keyword>Mathematics</keyword><keyword>Statistics</keyword><keyword>Statistik</keyword></keywords><dates><year>2016</year><pub-dates><date>2016</date></pub-dates></dates><pub-location>London ;Thousand Oaks, California</pub-location><publisher>SAGE Publications</publisher><isbn>978-1-4462-1044-4</isbn><abstract>Andy Field., Once again, bestselling, award-winning author and teacher Andy Field hasn't just broken the traditional textbook mould with his new novel/textbook, he has forged in the fire of his imagination the only statistics book on the market with a terrifying probability bridge, zombies and talking cats!.</abstract><remote-database-name>soeg.kb.dk</remote-database-name><language>eng</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Efron B</author><author>Tibshirani RJ</author><author>Efron Bradley</author><author>Tibshirani R J</author></authors></contributors><titles><title>An Introduction to the Bootstrap</title><secondary-title>Monographs on Statistics and Applied Probability 57.</secondary-title></titles><pages>436</pages><keywords><keyword>Bootstrap (Statistics)</keyword><keyword>Statistisk analyse</keyword><keyword>regressionsmodeller</keyword><keyword>sandsynlighed</keyword><keyword>statistik</keyword></keywords><dates><year>1993</year><pub-dates><date>1993</date></pub-dates></dates><publisher>Chapman &amp; Hall</publisher><isbn>978-0-412-04231-7</isbn><abstract>Efron, B,; Tibshirani, R.J.</abstract><remote-database-name>soeg.kb.dk</remote-database-name><language>eng</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Good, Phillip I.</author></authors></contributors><titles><title>Permutation, Parametric, and Bootstrap Tests of Hypotheses</title></titles><edition>3</edition><keywords><keyword>Resampling (Statistics)</keyword></keywords><dates><year>2004</year><pub-dates><date>2004</date></pub-dates></dates><pub-location>New York, NY</pub-location><publisher>Springer</publisher><isbn>978-0-387-20279-2</isbn><remote-database-name>soeg.kb.dk</remote-database-name><language>eng</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Poldrack, Copyright 2019 Russell A.</author></authors></contributors><titles><title>Statistical Thinking for the 21st Century</title></titles><dates/><abstract>A book about statistics.</abstract><remote-database-name>statsthinking21.github.io</remote-database-name><urls><web-urls><url>https://statsthinking21.github.io/statsthinking21-core-site/index.html</url></web-urls></urls><access-date>2023-10-17 08:07:27</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Szucs, Denes</author></authors></contributors><titles><title>A Tutorial on Hunting Statistical Significance by Chasing N</title><secondary-title>Frontiers in Psychology</secondary-title></titles><periodical><full-title>Frontiers in Psychology</full-title></periodical><volume>7</volume><dates><year>2016</year><pub-dates><date>2016</date></pub-dates></dates><isbn>1664-1078</isbn><abstract>There is increasing concern about the replicability of studies in psychology and cognitive neuroscience. Hidden data dredging (also called p-hacking) is a major contributor to this crisis because it substantially increases Type I error resulting in a much larger proportion of false positive findings than the usually expected 5%. In order to build better intuition to avoid, detect and criticize some typical problems, here I systematically illustrate the large impact of some easy to implement and so, perhaps frequent data dredging techniques on boosting false positive findings. I illustrate several forms of two special cases of data dredging. First, researchers may violate the data collection stopping rules of null hypothesis significance testing by repeatedly checking for statistical significance with various numbers of participants. Second, researchers may group participants post hoc along potential but unplanned independent grouping variables. The first approach ‘hacks’ the number of participants in studies, the second approach ‘hacks’ the number of variables in the analysis. I demonstrate the high amount of false positive findings generated by these techniques with data from true null distributions. I also illustrate that it is extremely easy to introduce strong bias into data by very mild selection and re-testing. Similar, usually undocumented data dredging steps can easily lead to having 20–50%, or more false positives.</abstract><remote-database-name>Frontiers</remote-database-name><urls><web-urls><url>https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01444</url></web-urls></urls><access-date>2023-10-17 07:53:31</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Nuzzo, Regina</author></authors></contributors><titles><title>How scientists fool themselves – and how they can stop</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><pages>182-185</pages><volume>526</volume><number>7572</number><issue>7572</issue><keywords><keyword>Lab life</keyword><keyword>Research management</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-10-01</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/526182a</electronic-resource-num><abstract>Humans are remarkably good at self-deception. But growing concern about reproducibility is driving many researchers to seek ways to fight their own worst instincts.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/526182a</url></web-urls></urls><access-date>2023-10-17 07:10:07</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Benjamini, Yoav</author><author>Hochberg, Yosef</author></authors></contributors><titles><title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</title><secondary-title>Journal of the Royal Statistical Society. Series B (Methodological)</secondary-title><short-title>Controlling the False Discovery Rate</short-title></titles><periodical><full-title>Journal of the Royal Statistical Society. Series B (Methodological)</full-title></periodical><pages>289-300</pages><volume>57</volume><number>1</number><issue>1</issue><dates><year>1995</year><pub-dates><date>1995</date></pub-dates></dates><isbn>0035-9246</isbn><abstract>The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.</abstract><remote-database-name>JSTOR</remote-database-name><urls><web-urls><url>https://www.jstor.org/stable/2346101</url></web-urls></urls><access-date>2023-10-17 07:48:18</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Banerjee, Amitav</author><author>Chitnis, U. B.</author><author>Jadhav, S. L.</author><author>Bhawalkar, J. S.</author><author>Chaudhury, S.</author></authors></contributors><titles><title>Hypothesis testing, type I and type II errors</title><secondary-title>Industrial Psychiatry Journal</secondary-title></titles><periodical><full-title>Industrial Psychiatry Journal</full-title><abbr-1>Ind Psychiatry J</abbr-1></periodical><pages>127-131</pages><volume>18</volume><number>2</number><issue>2</issue><dates><year>2009</year><pub-dates><date>2009</date></pub-dates></dates><isbn>0972-6748</isbn><electronic-resource-num>10.4103/0972-6748.62274</electronic-resource-num><abstract>Hypothesis testing is an important activity of empirical research and evidence-based medicine. A well worked up hypothesis is half the answer to the research question. For this, both knowledge of the subject derived from extensive review of the literature and working knowledge of basic statistical concepts are desirable. The present paper discusses the methods of working up a good hypothesis and statistical concepts of hypothesis testing.</abstract><remote-database-name>PubMed Central</remote-database-name><urls><web-urls><url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/</url></web-urls></urls><access-date>2023-10-17 07:29:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Report">27</ref-type><contributors><authors><author>Pernet, Cyril</author></authors></contributors><titles><title>Null hypothesis significance testing: a guide to commonly misunderstood concepts and recommendations for good practice</title><short-title>Null hypothesis significance testing</short-title></titles><keywords><keyword>confidence intervals</keyword><keyword>null hypothesis significance testing</keyword><keyword>p-value</keyword><keyword>reporting</keyword><keyword>tutorial</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-10-12</date></pub-dates></dates><publisher>F1000Research</publisher><isbn>4:621</isbn><abstract>Although thoroughly criticized, null hypothesis significance testing (NHST) remains the statistical method of choice used to provide evidence for an effect, in biological, biomedical and social sciences. In this short guide, I first summarize the concepts behind the method, distinguishing test of significance (Fisher) and test of acceptance (Newman-Pearson) and point to common interpretation errors regarding the p-value. I then present the related concepts of confidence intervals and again point to common interpretation errors. Finally, I discuss what should be reported in which context. The goal is to clarify concepts to avoid interpretation errors and propose simple reporting practices.</abstract><remote-database-name>f1000research.com</remote-database-name><language>en</language><urls><web-urls><url>https://f1000research.com/articles/4-621</url></web-urls></urls><access-date>2023-10-17 07:30:17</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Nickerson, R. S.</author></authors></contributors><titles><title>Null hypothesis significance testing: a review of an old and continuing controversy</title><secondary-title>Psychological Methods</secondary-title><short-title>Null hypothesis significance testing</short-title></titles><periodical><full-title>Psychological Methods</full-title><abbr-1>Psychol Methods</abbr-1></periodical><pages>241-301</pages><volume>5</volume><number>2</number><issue>2</issue><keywords><keyword>Data Interpretation, Statistical</keyword><keyword>Humans</keyword><keyword>Psychology, Experimental</keyword><keyword>Psychometrics</keyword><keyword>Reproducibility of Results</keyword></keywords><dates><year>2000</year><pub-dates><date>2000-06</date></pub-dates></dates><isbn>1082-989X</isbn><electronic-resource-num>10.1037/1082-989x.5.2.241</electronic-resource-num><abstract>Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data.</abstract><remote-database-name>PubMed</remote-database-name><language>eng</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Web Page">12</ref-type><contributors><authors><author>Weisstein, Eric W.</author></authors></contributors><titles><title>Bonferroni Correction</title></titles><dates/><abstract>The Bonferroni correction is a multiple-comparison correction used when several dependent or independent statistical tests are being performed simultaneously (since while a given alpha value alpha may be appropriate for each individual comparison, it is not for the set of all comparisons). In order to avoid a lot of spurious positives, the alpha value needs to be lowered to account for the number of comparisons being performed. The simplest and most conservative approach is the Bonferroni...</abstract><work-type>Text</work-type><language>en</language><urls><web-urls><url>https://mathworld.wolfram.com/</url></web-urls></urls><access-date>2023-10-17 07:51:56</access-date><misc2>Text</misc2></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><titles><title>Let’s think about cognitive bias</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><pages>163-163</pages><volume>526</volume><number>7572</number><issue>7572</issue><keywords><keyword>Research management</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-10</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/526163a</electronic-resource-num><abstract>The human brain’s habit of finding what it wants to find is a key problem for research. Establishing robust methods to avoid such bias will make results more reproducible.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/526163a</url></web-urls></urls><access-date>2023-10-17 07:49:52</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Charpentier, Arthur</author></authors></contributors><titles><title>p-hacking, or cheating on a p-value</title><secondary-title>Freakonometrics</secondary-title></titles><periodical><full-title>Freakonometrics</full-title></periodical><dates><year>2015</year><pub-dates><date>2015-06-11</date></pub-dates></dates><abstract>Yesterday evening, I discovered some interesting slides on False-Positives, p-Hacking, Statistical Power, and Evidential Value, via @UCBITSS ‘s post on Twitter. More precisely, there was this slide on how cheating (because that’s basically what it is) to get a ‘good’ model (by targeting the p-value) As mentioned by @david_colquhoun  one should be careful when reading the slides : some statistician … Continue reading p-hacking, or cheating on a p-value →</abstract><language>en-US</language><urls><web-urls><url>https://freakonometrics.hypotheses.org/19817</url></web-urls></urls><access-date>2023-10-17 07:53:50</access-date><misc2>Billet</misc2></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Eglen, Stephen J.</author><author>Marwick, Ben</author><author>Halchenko, Yaroslav O.</author><author>Hanke, Michael</author><author>Sufi, Shoaib</author><author>Gleeson, Padraig</author><author>Silver, R. Angus</author><author>Davison, Andrew P.</author><author>Lanyon, Linda</author><author>Abrams, Mathew</author><author>Wachtler, Thomas</author><author>Willshaw, David J.</author><author>Pouzat, Christophe</author><author>Poline, Jean-Baptiste</author></authors></contributors><titles><title>Toward standard practices for sharing computer code and programs in neuroscience</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title><abbr-1>Nat Neurosci</abbr-1></periodical><pages>770-773</pages><volume>20</volume><number>6</number><issue>6</issue><keywords><keyword>Neuroscience</keyword><keyword>Scientific community</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-06</date></pub-dates></dates><isbn>1546-1726</isbn><electronic-resource-num>10.1038/nn.4550</electronic-resource-num><abstract>Computational techniques are central in many areas of neuroscience and are relatively easy to share. This paper describes why computer programs underlying scientific publications should be shared and lists simple steps for sharing. Together with ongoing efforts in data sharing, this should aid reproducibility of research.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nn.4550</url></web-urls></urls><access-date>2023-10-17 07:54:16</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Bannier, Elise</author><author>Barker, Gareth</author><author>Borghesani, Valentina</author><author>Broeckx, Nils</author><author>Clement, Patricia</author><author>Emblem, Kyrre E.</author><author>Ghosh, Satrajit</author><author>Glerean, Enrico</author><author>Gorgolewski, Krzysztof J.</author><author>Havu, Marko</author><author>Halchenko, Yaroslav O.</author><author>Herholz, Peer</author><author>Hespel, Anne</author><author>Heunis, Stephan</author><author>Hu, Yue</author><author>Hu, Chuan-Peng</author><author>Huijser, Dorien</author><author>de la Iglesia Vayá, María</author><author>Jancalek, Radim</author><author>Katsaros, Vasileios K.</author><author>Kieseler, Marie-Luise</author><author>Maumet, Camille</author><author>Moreau, Clara A.</author><author>Mutsaerts, Henk-Jan</author><author>Oostenveld, Robert</author><author>Ozturk-Isik, Esin</author><author>Pascual Leone Espinosa, Nicolas</author><author>Pellman, John</author><author>Pernet, Cyril R</author><author>Pizzini, Francesca Benedetta</author><author>Trbalić, Amira Šerifović</author><author>Toussaint, Paule-Joanne</author><author>Visconti di Oleggio Castello, Matteo</author><author>Wang, Fengjuan</author><author>Wang, Cheng</author><author>Zhu, Hua</author></authors></contributors><titles><title>The Open Brain Consent: Informing research participants and obtaining consent to share brain imaging data</title><secondary-title>Human Brain Mapping</secondary-title><short-title>The Open Brain Consent</short-title></titles><periodical><full-title>Human Brain Mapping</full-title></periodical><pages>1945-1951</pages><volume>42</volume><number>7</number><issue>7</issue><keywords><keyword>brain imaging</keyword><keyword>general data protection regulation</keyword><keyword>informed consent</keyword></keywords><dates><year>2021</year><pub-dates><date>2021</date></pub-dates></dates><isbn>1097-0193</isbn><electronic-resource-num>10.1002/hbm.25351</electronic-resource-num><abstract>Having the means to share research data openly is essential to modern science. For human research, a key aspect in this endeavor is obtaining consent from participants, not just to take part in a study, which is a basic ethical principle, but also to share their data with the scientific community. To ensure that the participants' privacy is respected, national and/or supranational regulations and laws are in place. It is, however, not always clear to researchers what the implications of those are, nor how to comply with them. The Open Brain Consent (https://open-brain-consent.readthedocs.io) is an international initiative that aims to provide researchers in the brain imaging community with information about data sharing options and tools. We present here a short history of this project and its latest developments, and share pointers to consent forms, including a template consent form that is compliant with the EU general data protection regulation. We also share pointers to an associated data user agreement that is not only useful in the EU context, but also for any researchers dealing with personal (clinical) data elsewhere.</abstract><remote-database-name>Wiley Online Library</remote-database-name><language>en</language><urls><web-urls><url>https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.25351</url></web-urls></urls><access-date>2023-06-27 09:12:31</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Allen, M David</author><author>Seligman, Len</author><author>Blaustein, Barbara</author><author>Chapman, Adriane</author></authors></contributors><titles><title>Provenance Capture and Use: A Practical Guide</title></titles><dates><year>2010</year><pub-dates><date>2010</date></pub-dates></dates><abstract>There is a widespread recognition across MITRE’s sponsors of the importance of capturing the provenance of information (sometimes called lineage or pedigree). However, the technology for supporting capture and usage of provenance is relatively immature. While there has been much research, few commercial capabilities exist. In addition, there is neither a commonly understood concept of operations nor established best practices for how to capture and use provenance information in a consistent and principled way. This document captures lessons learned from the IM-PLUS project, which is prototyping a provenance capability that synthesizes prior research; the project is also applying the prototype to government application scenarios spanning defense, homeland security, and bio-surveillance. We describe desirable features of a provenance capability and trade-offs among alternate provenance approaches. The target audience is systems engineers and information architects advising our sponsors on how to improve their information management.</abstract><remote-database-name>Zotero</remote-database-name><language>en</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Vukovic, Nikola</author></authors></contributors><titles><title>Setting up an Organised Folder Structure for Research Projects</title></titles><dates/><urls><web-urls><url>http://nikola.me/folder_structure.html</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><titles><title>GIN-TONIC: a digital shelf for your research files</title></titles><dates/></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Glerean, Enrico</author></authors></contributors><titles><title>Project management == Data management</title></titles><dates/><urls><web-urls><url>https://eglerean.wordpress.com/2017/05/24/project-management-data-management/</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Munafò, Marcus R.</author><author>Nosek, Brian A.</author><author>Bishop, Dorothy V. M.</author><author>Button, Katherine S.</author><author>Chambers, Christopher D.</author><author>Percie Du Sert, Nathalie</author><author>Simonsohn, Uri</author><author>Wagenmakers, Eric-Jan</author><author>Ware, Jennifer J.</author><author>Ioannidis, John P. A.</author></authors></contributors><titles><title>A manifesto for reproducible science</title><secondary-title>Nature Human Behaviour</secondary-title></titles><periodical><full-title>Nature Human Behaviour</full-title><abbr-1>Nat Hum Behav</abbr-1></periodical><pages>0021</pages><volume>1</volume><number>1</number><issue>1</issue><dates><year>2017</year><pub-dates><date>2017-01-10</date></pub-dates></dates><isbn>2397-3374</isbn><electronic-resource-num>10.1038/s41562-016-0021</electronic-resource-num><abstract>Abstract&#xD;            Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/s41562-016-0021</url></web-urls></urls><access-date>2023-06-27 08:08:34</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Fernández-Juricic, Esteban</author></authors></contributors><titles><title>Why sharing data and code during peer review can enhance behavioral ecology research</title><secondary-title>Behavioral Ecology and Sociobiology</secondary-title></titles><periodical><full-title>Behavioral Ecology and Sociobiology</full-title><abbr-1>Behav Ecol Sociobiol</abbr-1></periodical><pages>103, s00265-021-03036-x</pages><volume>75</volume><number>7</number><issue>7</issue><dates><year>2021</year><pub-dates><date>07/2021</date></pub-dates></dates><isbn>0340-5443, 1432-0762</isbn><electronic-resource-num>10.1007/s00265-021-03036-x</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://link.springer.com/10.1007/s00265-021-03036-x</url></web-urls></urls><access-date>2023-06-27 08:07:55</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Galton, Francis</author></authors></contributors><titles><title>Biometry</title><secondary-title>Biometrika</secondary-title></titles><periodical><full-title>Biometrika</full-title></periodical><pages>7-10</pages><volume>1</volume><dates><year>1901</year><pub-dates><date>1901</date></pub-dates></dates></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>King, Gary</author></authors></contributors><titles><title>Publication, Publication</title></titles><dates><year>2006</year><pub-dates><date>2006</date></pub-dates></dates><remote-database-name>Zotero</remote-database-name><language>en</language></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Web Page">12</ref-type><titles><title>A survey on how preregistration affects the research workflow: better science but more work | Royal Society Open Science</title></titles><dates/><urls><web-urls><url>https://royalsocietypublishing.org/doi/full/10.1098/rsos.211997</url></web-urls></urls><access-date>2023-06-27 08:02:55</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Book">6</ref-type><contributors><authors><author>Fisher, Ronald A.</author></authors></contributors><titles><title>The design of experiments</title></titles><edition>9</edition><dates><year>1971</year><pub-dates><date>1971</date></pub-dates></dates></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Markowetz, Florian</author></authors></contributors><titles><title>Five selfish reasons to work reproducibly</title><secondary-title>Genome Biology</secondary-title></titles><periodical><full-title>Genome Biology</full-title><abbr-1>Genome Biol</abbr-1></periodical><pages>274</pages><volume>16</volume><number>1</number><issue>1</issue><dates><year>2015</year><pub-dates><date>12/2015</date></pub-dates></dates><isbn>1474-760X</isbn><electronic-resource-num>10.1186/s13059-015-0850-7</electronic-resource-num><abstract>And so, my fellow scientists: ask not what you can do for reproducibility; ask what reproducibility can do for you! Here, I present five reasons why working reproducibly pays off in the long run and is in the self-interest of every ambitious, career-oriented scientist.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7</url></web-urls></urls><access-date>2023-06-27 07:57:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Colavizza, Giovanni</author><author>Hrynaszkiewicz, Iain</author><author>Staden, Isla</author><author>Whitaker, Kirstie</author><author>McGillivray, Barbara</author></authors><secondary-authors><author>Wicherts, Jelte M.</author></secondary-authors></contributors><titles><title>The citation advantage of linking publications to research data</title><secondary-title>PLOS ONE</secondary-title></titles><periodical><full-title>PLOS ONE</full-title><abbr-1>PLoS ONE</abbr-1></periodical><pages>e0230416</pages><volume>15</volume><number>4</number><issue>4</issue><dates><year>2020</year><pub-dates><date>2020-4-22</date></pub-dates></dates><isbn>1932-6203</isbn><electronic-resource-num>10.1371/journal.pone.0230416</electronic-resource-num><abstract>Efforts to make research results open and reproducible are increasingly reflected by journal policies encouraging or mandating authors to provide data availability statements. As a consequence of this, there has been a strong uptake of data availability statements in recent literature. Nevertheless, it is still unclear what proportion of these statements actually contain well-formed links to data, for example via a URL or permanent identifier, and if there is an added value in providing such links. We consider 531, 889 journal articles published by PLOS and BMC, develop an automatic system for labelling their data availability statements according to four categories based on their content and the type of data availability they display, and finally analyze the citation advantage of different statement categories via regression. We find that, following mandated publisher policies, data availability statements become very common. In 2018 93.7% of 21,793 PLOS articles and 88.2% of 31,956 BMC articles had data availability statements. Data availability statements containing a link to data in a repository—rather than being available on request or included as supporting information files—are a fraction of the total. In 2017 and 2018, 20.8% of PLOS publications and 12.2% of BMC publications provided DAS containing a link to data in a repository. We also find an association between articles that include statements that link to data in a repository and up to 25.36% (± 1.07%) higher citation impact on average, using a citation prediction model. We discuss the potential implications of these results for authors (researchers) and journal publishers who make the effort of sharing their data in repositories. All our data and code are made available in order to reproduce and extend our results.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://dx.plos.org/10.1371/journal.pone.0230416</url></web-urls></urls><access-date>2023-06-27 07:55:56</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Pernet, Cyril</author><author>Poline, Jean-Baptiste</author></authors></contributors><titles><title>Improving functional magnetic resonance imaging reproducibility</title><secondary-title>GigaScience</secondary-title></titles><periodical><full-title>GigaScience</full-title><abbr-1>GigaSci</abbr-1></periodical><pages>15</pages><volume>4</volume><number>1</number><issue>1</issue><dates><year>2015</year><pub-dates><date>12/2015</date></pub-dates></dates><isbn>2047-217X</isbn><electronic-resource-num>10.1186/s13742-015-0055-8</electronic-resource-num><abstract>Background: The ability to replicate an entire experiment is crucial to the scientific method. With the development of more and more complex paradigms, and the variety of analysis techniques available, fMRI studies are becoming harder to reproduce.&#xD;Results: In this article, we aim to provide practical advice to fMRI researchers not versed in computing, in order to make studies more reproducible. All of these steps require researchers to move towards a more open science, in which all aspects of the experimental method are documented and shared.&#xD;Conclusion: Only by sharing experiments, data, metadata, derived data and analysis workflows will neuroimaging establish itself as a true data science.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://academic.oup.com/gigascience/article-lookup/doi/10.1186/s13742-015-0055-8</url></web-urls></urls><access-date>2023-06-27 07:55:20</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Whitacker, Kirstie</author></authors></contributors><titles><title>The Turing Way: A Handbook for Reproducible Data Science</title></titles><dates/><urls><web-urls><url>https://the-turing-way.netlify.app/index.html</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poldrack, Russell A.</author><author>Baker, Chris I.</author><author>Durnez, Joke</author><author>Gorgolewski, Krzysztof J.</author><author>Matthews, Paul M.</author><author>Munafò, Marcus R.</author><author>Nichols, Thomas E.</author><author>Poline, Jean-Baptiste</author><author>Vul, Edward</author><author>Yarkoni, Tal</author></authors></contributors><titles><title>Scanning the horizon: towards transparent and reproducible neuroimaging research</title><secondary-title>Nature Reviews Neuroscience</secondary-title></titles><periodical><full-title>Nature Reviews Neuroscience</full-title></periodical><pages>115-126</pages><volume>18</volume><number>2</number><issue>2</issue><keywords><keyword>★</keyword></keywords><dates><year>2017</year><pub-dates><date>2017</date></pub-dates></dates><isbn>1471-003X</isbn><electronic-resource-num>10.1038/nrn.2016.167</electronic-resource-num><abstract>Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors, and lack of direct replication apply to many fields, but perhaps particularly to fMRI. Here we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful answers to neuroscientific questions.</abstract><urls><web-urls><url>http://www.nature.com/doifinder/10.1038/nrn.2016.167</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Report">27</ref-type><contributors><authors><author>Poldrack, Russell</author></authors></contributors><titles><title>Reproducibility in neuroimaging: Challenges and solutions</title></titles><dates/></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poldrack, Russell A.</author><author>Huckins, Grace</author><author>Varoquaux, Gael</author></authors></contributors><titles><title>Establishment of Best Practices for Evidence for Prediction: A Review</title><secondary-title>JAMA Psychiatry</secondary-title></titles><periodical><full-title>JAMA Psychiatry</full-title></periodical><dates><year>2020</year><pub-dates><date>2020</date></pub-dates></dates><isbn>2168622X</isbn><electronic-resource-num>10.1001/jamapsychiatry.2019.3671</electronic-resource-num><abstract>Importance: Great interest exists in identifying methods to predict neuropsychiatric disease states and treatment outcomes from high-dimensional data, including neuroimaging and genomics data. The goal of this review is to highlight several potential problems that can arise in studies that aim to establish prediction. Observations: A number of neuroimaging studies have claimed to establish prediction while establishing only correlation, which is an inappropriate use of the statistical meaning of prediction. Statistical associations do not necessarily imply the ability to make predictions in a generalized manner; establishing evidence for prediction thus requires testing of the model on data separate from those used to estimate the model's parameters. This article discusses various measures of predictive performance and the limitations of some commonly used measures, with a focus on the importance of using multiple measures when assessing performance. For classification, the area under the receiver operating characteristic curve is an appropriate measure; for regression analysis, correlation should be avoided, and median absolute error is preferred. Conclusions and Relevance: To ensure accurate estimates of predictive validity, the recommended best practices for predictive modeling include the following: (1) in-sample model fit indices should not be reported as evidence for predictive accuracy, (2) the cross-validation procedure should encompass all operations applied to the data, (3) prediction analyses should not be performed with samples smaller than several hundred observations, (4) multiple measures of prediction accuracy should be examined and reported, (5) the coefficient of determination should be computed using the sums of squares formulation and not the correlation coefficient, and (6) k-fold cross-validation rather than leave-one-out cross-validation should be used..</abstract></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Halchenko, Yaroslav</author><author>Meyer, Kyle</author><author>Poldrack, Benjamin</author><author>Solanky, Debanjum</author><author>Wagner, Adina</author><author>Gors, Jason</author><author>MacFarlane, Dave</author><author>Pustina, Dorian</author><author>Sochat, Vanessa</author><author>Ghosh, Satrajit</author><author>Mönch, Christian</author><author>Markiewicz, Christopher</author><author>Waite, Laura</author><author>Shlyakhter, Ilya</author><author>de la Vega, Alejandro</author><author>Hayashi, Soichi</author><author>Häusler, Christian</author><author>Poline, Jean-Baptiste</author><author>Kadelka, Tobias</author><author>Skytén, Kusti</author><author>Jarecka, Dorota</author><author>Kennedy, David</author><author>Strauss, Ted</author><author>Cieslak, Matt</author><author>Vavra, Peter</author><author>Ioanas, Horea-Ioan</author><author>Schneider, Robin</author><author>Pflüger, Mika</author><author>Haxby, James</author><author>Eickhoff, Simon</author><author>Hanke, Michael</author></authors></contributors><titles><title>DataLad: distributed system for joint management of code, data, and their relationship</title><secondary-title>Journal of Open Source Software</secondary-title></titles><periodical><full-title>Journal of Open Source Software</full-title></periodical><pages>3262</pages><volume>6</volume><number>63</number><issue>63</issue><dates><year>2021</year><pub-dates><date>2021-07-01</date></pub-dates></dates><electronic-resource-num>10.21105/JOSS.03262</electronic-resource-num><abstract>… technical challenges of data management, data sharing, and digital provenance collection across the life cycle of digital objects. DataLad aims to make data management as easy as …</abstract><access-date>2022-04-06 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poldrack, Russell A.</author><author>Gorgolewski, Krzysztof J.</author></authors></contributors><titles><title>Making big data open: Data sharing in neuroimaging</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title></periodical><pages>1510-1517</pages><volume>17</volume><number>11</number><issue>11</issue><dates><year>2014</year><pub-dates><date>2014-10-28</date></pub-dates></dates><isbn>15461726</isbn><electronic-resource-num>10.1038/NN.3818</electronic-resource-num><abstract>In the last decade, major advances have been made in the availability of shared neuroimaging data, such that there are more than 8,000 shared MRI (magnetic resonance imaging) data sets available online. Here we outline the state of data sharing for task-based functional MRI (fMRI) data, with a focus on various forms of data and their relative utility for subsequent analyses. We also discuss challenges to the future success of data sharing and highlight the ethical argument that data sharing may be necessary to maximize the contribution of human subjects.</abstract><access-date>2022-05-01 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Poldrack, Russell A.</author></authors></contributors><titles><title>The Costs of Reproducibility</title><secondary-title>Neuron</secondary-title></titles><periodical><full-title>Neuron</full-title></periodical><pages>11-14</pages><volume>101</volume><number>1</number><issue>1</issue><dates><year>2019</year><pub-dates><date>2019-01-02</date></pub-dates></dates><isbn>0896-6273</isbn><electronic-resource-num>10.1016/J.NEURON.2018.11.030</electronic-resource-num><abstract>Improving the reproducibility of neuroscience research is of great concern, especially to early-career researchers (ECRs). Here I outline the potential costs for ECRs in adopting practices to improve reproducibility. I highlight the ways in which ECRs can achieve their career goals while doing better science and the need for established researchers to support them in these efforts.</abstract><access-date>2022-07-25 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Goodman, Steven N.</author><author>Fanelli, Daniele</author><author>Ioannidis, John P. A.</author></authors></contributors><titles><title>What does research reproducibility mean?</title><secondary-title>Science Translational Medicine</secondary-title></titles><periodical><full-title>Science Translational Medicine</full-title><abbr-1>Sci. Transl. Med.</abbr-1></periodical><volume>8</volume><number>341</number><issue>341</issue><dates><year>2016</year><pub-dates><date>06/2016</date></pub-dates></dates><isbn>1946-6234, 1946-6242</isbn><electronic-resource-num>10.1126/scitranslmed.aaf5027</electronic-resource-num><abstract>The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences.&#xD;          , &#xD;            The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://www.science.org/doi/10.1126/scitranslmed.aaf5027</url></web-urls></urls><access-date>2023-06-27 07:49:41</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Kenett, Ron S</author><author>Shmueli, Galit</author></authors></contributors><titles><title>Clarifying the terminology that describes scientific reproducibility</title><secondary-title>Nature Methods</secondary-title></titles><periodical><full-title>Nature Methods</full-title><abbr-1>Nat Methods</abbr-1></periodical><pages>699-699</pages><volume>12</volume><number>8</number><issue>8</issue><dates><year>2015</year><pub-dates><date>8/2015</date></pub-dates></dates><isbn>1548-7091, 1548-7105</isbn><electronic-resource-num>10.1038/nmeth.3489</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nmeth.3489</url></web-urls></urls><access-date>2023-06-27 07:48:31</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Baker, M</author></authors></contributors><titles><title>1,500 scientists lift the lid on reproducibility</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><dates><year>2016</year><pub-dates><date>2016</date></pub-dates></dates><electronic-resource-num>10.1038/533452a</electronic-resource-num></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Button, Katherine S.</author></authors></contributors><titles><title>Double-dipping revisited</title><secondary-title>Nature Neuroscience</secondary-title></titles><periodical><full-title>Nature Neuroscience</full-title></periodical><pages>688-690</pages><volume>22</volume><number>5</number><issue>5</issue><dates><year>2019</year><pub-dates><date>2019</date></pub-dates></dates><isbn>15461726</isbn><electronic-resource-num>10.1038/s41593-019-0398-z</electronic-resource-num><abstract>Robust conclusions require rigorous statistics. In 2009 a seminal paper described the dangers and prevalence of double-dipping in neuroscience. Ten years on, I consider progress toward statistical rigor in neuroimaging.</abstract><urls><web-urls><url>http://dx.doi.org/10.1038/s41593-019-0398-z</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Button, Katherine S.</author><author>Ioannidis, John P.A.</author><author>Mokrysz, Claire</author><author>Nosek, Brian A.</author><author>Flint, Jonathan</author><author>Robinson, Emma S.J.</author><author>Munafò, Marcus R.</author></authors></contributors><titles><title>Power failure: Why small sample size undermines the reliability of neuroscience</title><secondary-title>Nature Reviews Neuroscience</secondary-title></titles><periodical><full-title>Nature Reviews Neuroscience</full-title></periodical><pages>365-376</pages><volume>14</volume><number>5</number><issue>5</issue><dates><year>2013</year><pub-dates><date>2013-05</date></pub-dates></dates><isbn>1471003X</isbn><electronic-resource-num>10.1038/nrn3475</electronic-resource-num><abstract>A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles. © 2013 Macmillan Publishers Limited. All rights reserved.</abstract><access-date>2020-07-31 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Munafò, Marcus R.</author><author>Nosek, Brian A.</author><author>Bishop, Dorothy V.M.</author><author>Button, Katherine S.</author><author>Chambers, Christopher D.</author><author>Percie Du Sert, Nathalie</author><author>Simonsohn, Uri</author><author>Wagenmakers, Eric Jan</author><author>Ware, Jennifer J.</author><author>Ioannidis, John P.A.</author></authors></contributors><titles><title>A manifesto for reproducible science</title><secondary-title>Nature Human Behaviour</secondary-title></titles><periodical><full-title>Nature Human Behaviour</full-title></periodical><volume>1</volume><number>1</number><issue>1</issue><keywords><keyword>★</keyword></keywords><dates><year>2017</year><pub-dates><date>2017-01-10</date></pub-dates></dates><isbn>23973374</isbn><electronic-resource-num>10.1038/s41562-016-0021</electronic-resource-num><abstract>Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: Methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.</abstract><access-date>2020-07-31 00:00:00</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Baker, Monya</author></authors></contributors><titles><title>Over half of psychology studies fail reproducibility test</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title></periodical><keywords><keyword>Authorship</keyword><keyword>Psychology</keyword><keyword>Research management</keyword></keywords><dates><year>2015</year><pub-dates><date>2015-08-27</date></pub-dates></dates><isbn>1476-4687</isbn><electronic-resource-num>10.1038/nature.2015.18248</electronic-resource-num><abstract>Largest replication study to date casts doubt on many published positive results.</abstract><remote-database-name>www.nature.com</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nature.2015.18248</url></web-urls></urls><access-date>2023-01-23 09:20:44</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Open Science Collaboration</author></authors></contributors><titles><title>Estimating the reproducibility of psychological science</title><secondary-title>Science</secondary-title></titles><periodical><full-title>Science</full-title><abbr-1>Science</abbr-1></periodical><pages>aac4716</pages><volume>349</volume><number>6251</number><issue>6251</issue><dates><year>2015</year><pub-dates><date>2015-08-28</date></pub-dates></dates><isbn>0036-8075, 1095-9203</isbn><electronic-resource-num>10.1126/science.aac4716</electronic-resource-num><abstract>Empirically analyzing empirical evidence&#xD;            &#xD;              One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts&#xD;              et al.&#xD;              describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.&#xD;            &#xD;            &#xD;              Science&#xD;              , this issue&#xD;              10.1126/science.aac4716&#xD;            &#xD;          , &#xD;            A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.&#xD;          , &#xD;            &#xD;              INTRODUCTION&#xD;              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.&#xD;            &#xD;            &#xD;              RATIONALE&#xD;              There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.&#xD;            &#xD;            &#xD;              RESULTS&#xD;              &#xD;                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and&#xD;                P&#xD;                values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (&#xD;                M&#xD;                r&#xD;                = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (&#xD;                M&#xD;                r&#xD;                = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (&#xD;                P&#xD;                &lt; .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.&#xD;              &#xD;            &#xD;            &#xD;              CONCLUSION&#xD;              &#xD;                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original&#xD;                P&#xD;                value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.&#xD;              &#xD;              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.&#xD;              &#xD;                &#xD;                  Original study effect size versus replication effect size (correlation coefficients).&#xD;                  Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.&#xD;                &#xD;                &#xD;              &#xD;            &#xD;          , &#xD;            Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://www.science.org/doi/10.1126/science.aac4716</url></web-urls></urls><access-date>2023-06-27 07:42:51</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Errington, Timothy M</author><author>Mathur, Maya</author><author>Soderberg, Courtney K</author><author>Denis, Alexandria</author><author>Perfito, Nicole</author><author>Iorns, Elizabeth</author><author>Nosek, Brian A</author></authors></contributors><titles><title>Investigating the replicability of preclinical cancer biology</title><secondary-title>eLife</secondary-title></titles><periodical><full-title>eLife</full-title></periodical><pages>e71601</pages><volume>10</volume><dates><year>2021</year><pub-dates><date>2021-12-10</date></pub-dates></dates><isbn>2050-084X</isbn><electronic-resource-num>10.7554/eLife.71601</electronic-resource-num><abstract>Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-­ impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85% smaller than the median effect size in the original experiments, and 92% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://elifesciences.org/articles/71601</url></web-urls></urls><access-date>2023-06-27 07:42:22</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Kleinert, Sabine</author><author>Horton, Richard</author></authors></contributors><titles><title>How should medical science change?</title><secondary-title>The Lancet</secondary-title></titles><periodical><full-title>The Lancet</full-title><abbr-1>The Lancet</abbr-1></periodical><pages>197-198</pages><volume>383</volume><number>9913</number><issue>9913</issue><dates><year>2014</year><pub-dates><date>01/2014</date></pub-dates></dates><isbn>01406736</isbn><electronic-resource-num>10.1016/S0140-6736(13)62678-1</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://linkinghub.elsevier.com/retrieve/pii/S0140673613626781</url></web-urls></urls><access-date>2023-06-27 07:39:48</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Macleod, Malcolm R</author><author>Michie, Susan</author><author>Roberts, Ian</author><author>Dirnagl, Ulrich</author><author>Chalmers, Iain</author><author>Ioannidis, John P A</author><author>Salman, Rustam Al-Shahi</author><author>Chan, An-Wen</author><author>Glasziou, Paul</author></authors></contributors><titles><title>Biomedical research: increasing value, reducing waste</title><secondary-title>The Lancet</secondary-title><short-title>Biomedical research</short-title></titles><periodical><full-title>The Lancet</full-title><abbr-1>The Lancet</abbr-1></periodical><pages>101-104</pages><volume>383</volume><number>9912</number><issue>9912</issue><dates><year>2014</year><pub-dates><date>01/2014</date></pub-dates></dates><isbn>01406736</isbn><electronic-resource-num>10.1016/S0140-6736(13)62329-6</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://linkinghub.elsevier.com/retrieve/pii/S0140673613623296</url></web-urls></urls><access-date>2023-06-27 07:39:42</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Bustin, Stephen A.</author></authors></contributors><titles><title>The reproducibility of biomedical research: Sleepers awake!</title><secondary-title>Biomolecular Detection and Quantification</secondary-title><short-title>The reproducibility of biomedical research</short-title></titles><periodical><full-title>Biomolecular Detection and Quantification</full-title><abbr-1>Biomolecular Detection and Quantification</abbr-1></periodical><pages>35-42</pages><volume>2</volume><dates><year>2014</year><pub-dates><date>12/2014</date></pub-dates></dates><isbn>22147535</isbn><electronic-resource-num>10.1016/j.bdq.2015.01.002</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://linkinghub.elsevier.com/retrieve/pii/S2214753515000030</url></web-urls></urls><access-date>2023-06-27 07:38:11</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Baker, Monya</author></authors></contributors><titles><title>First results from psychology’s largest reproducibility test</title><secondary-title>Nature</secondary-title></titles><periodical><full-title>Nature</full-title><abbr-1>Nature</abbr-1></periodical><pages>nature.2015.17433</pages><dates><year>2015</year><pub-dates><date>2015-4-30</date></pub-dates></dates><isbn>0028-0836, 1476-4687</isbn><electronic-resource-num>10.1038/nature.2015.17433</electronic-resource-num><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://www.nature.com/articles/nature.2015.17433</url></web-urls></urls><access-date>2023-06-27 07:37:19</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Ioannidis, John P. A.</author></authors></contributors><titles><title>Why Most Published Research Findings Are False</title><secondary-title>PLoS Medicine</secondary-title></titles><periodical><full-title>PLoS Medicine</full-title><abbr-1>PLoS Med</abbr-1></periodical><pages>e124</pages><volume>2</volume><number>8</number><issue>8</issue><dates><year>2005</year><pub-dates><date>2005-8-30</date></pub-dates></dates><isbn>1549-1676</isbn><electronic-resource-num>10.1371/journal.pmed.0020124</electronic-resource-num><abstract>There is increasing concern that most current published research ﬁndings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientiﬁc ﬁeld. In this framework, a research ﬁnding is less likely to be true when the studies conducted in a ﬁeld are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater ﬂexibility in designs, deﬁnitions, outcomes, and analytical modes; when there is greater ﬁnancial and other interest and prejudice; and when more teams are involved in a scientiﬁc ﬁeld in chase of statistical signiﬁcance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientiﬁc ﬁelds, claimed research ﬁndings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://dx.plos.org/10.1371/journal.pmed.0020124</url></web-urls></urls><access-date>2023-06-27 07:36:37</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Nosek, Brian</author></authors></contributors><titles><title>Breakdown of the percentage of papers mentioning preregistration in sub-disciplines of psychology.</title></titles><dates/><urls><web-urls><url>https://nerdculture.de/@briannosek/110533439789757366</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Sarafoglou, Alexandra</author><author>Kovacs, Marton</author><author>Bakos, Bence</author><author>Wagenmakers, Eric-Jan</author><author>Aczel, Balazs</author></authors></contributors><titles><title>A survey on how preregistration affects the research workflow: better science but more work</title><secondary-title>Royal Society Open Science</secondary-title><short-title>A survey on how preregistration affects the research workflow</short-title></titles><periodical><full-title>Royal Society Open Science</full-title><abbr-1>R. Soc. open sci.</abbr-1></periodical><pages>211997</pages><volume>9</volume><number>7</number><issue>7</issue><dates><year>2022</year><pub-dates><date>07/2022</date></pub-dates></dates><isbn>2054-5703</isbn><electronic-resource-num>10.1098/rsos.211997</electronic-resource-num><abstract>The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.</abstract><remote-database-name>DOI.org (Crossref)</remote-database-name><language>en</language><urls><web-urls><url>https://royalsocietypublishing.org/doi/10.1098/rsos.211997</url></web-urls></urls><access-date>2023-06-27 07:31:49</access-date></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Blog">56</ref-type><contributors><authors><author>Blattman, Chris</author></authors></contributors><titles><title>Preregistration of clinical trials causes medicines to stop working!</title></titles><dates/><urls><web-urls><url>https://chrisblattman.com/blog/2016/03/01/13719/</url></web-urls></urls></record><record><database name="MyLibrary">MyLibrary</database><source-app name="Zotero">Zotero</source-app><ref-type name="Journal Article">17</ref-type><contributors><authors><author>Kwok, Roberta</author><author>Kanza, S</author></authors></contributors><titles><title>A burgeoning array of digital tools is helping researchers to document experiments with ease.</title></titles><dates/><remote-database-name>Zotero</remote-database-name><language>en</language></record></records></xml>